{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyP3ta9AhNxv3L35hEZxtoU2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rprimi/b5_nlp/blob/main/m2_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pacakges <- c(\"tidyverse\", \"keras\", \"psych\")\n",
        "install.packages(pacakges)\n",
        "\n",
        "install.packages(\"readr\")\n",
        "install.packages(\"zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zu3HAEog7BsJ",
        "outputId": "e3597acc-e437-475b-8846-74582bb79d12"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMnn2iO95jpK",
        "outputId": "b1cb1015-5c10-436b-ca35-aebd1a9ccc58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Attaching package: ‘zip’\n",
            "\n",
            "\n",
            "The following objects are masked from ‘package:utils’:\n",
            "\n",
            "    unzip, zip\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "library(tidyverse)\n",
        "library(keras)\n",
        "library(psych)\n",
        "library(readr)\n",
        "library(zip)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load(\"base_bfi_nlp.RData\")"
      ],
      "metadata": {
        "id": "u179Fq4m8HUe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples <- db_textos_splited$texto_dividido\n",
        "\n",
        "tokenizer <- text_tokenizer(\n",
        "    lower = TRUE )  %>%\n",
        "    fit_text_tokenizer(samples)\n",
        "\n",
        "word_index <- tokenizer$word_index\n",
        "\n",
        "sequences <- texts_to_sequences(tokenizer, samples)\n",
        "\n",
        "cat(\"Found\", length(word_index), \"unique tokens.\\n\")\n",
        ""
      ],
      "metadata": {
        "id": "JpE1zzRI8YIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load(\"/content/vocabulary.RData\")\n",
        "names(vocabulary)\n",
        "\n",
        "# Function adapted from Chalot book\n",
        "  prepare_embedding_matrix <- function(num_words, EMBEDDING_DIM, word_index) {\n",
        "      MAX_NUM_WORDS = num_words\n",
        "      embedding_matrix <- matrix(0L, nrow = num_words+1, ncol = EMBEDDING_DIM)\n",
        "      for (word in names(word_index)) {\n",
        "        index <- word_index[[word]]\n",
        "        if (index >= MAX_NUM_WORDS)\n",
        "          next\n",
        "        embedding_vector <- as.numeric(vocabulary[vocabulary$word == word, 3:602])\n",
        "        if (!is.null(embedding_vector)) {\n",
        "          # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[index+1,] <- embedding_vector\n",
        "        }\n",
        "      }\n",
        "      embedding_matrix\n",
        "    }\n",
        "\n",
        "# Creates embedding matrix\n",
        "     embedding_matrix <- prepare_embedding_matrix(\n",
        "         num_words = 66128,\n",
        "         EMBEDDING_DIM = 600,\n",
        "         word_index = word_index)\n",
        "\n",
        "# Fills empty rows with random number\n",
        "    rnd <- runif(n=sum(is.na(embedding_matrix)), min = 0, max = .04)\n",
        "    embedding_matrix[is.na(embedding_matrix)] <- rnd\n",
        "\n",
        "# Test it\n",
        "   table(is.na(embedding_matrix))\n",
        "\n",
        "# See shape of embedding matrix and first row (needs to be zero, don't know why)\n",
        "# https://github.com/rstudio/keras/issues/302)\n",
        "   dim(embedding_matrix)\n",
        "   embedding_matrix[1, ]\n",
        ""
      ],
      "metadata": {
        "id": "zzM0nd-D-ver"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    hist(map_dbl(sequences, length))\n",
        "\n",
        "# Shuffle data\n",
        "    set.seed(8)\n",
        "\n",
        "    indices <- sample(1:length(sequences))\n",
        "\n",
        "    prop_train <- .82\n",
        "    maxlen <-250\n",
        "\n",
        "    train_indices <- 1: round(prop_train*length(sequences), 0)\n",
        "    test_indices <-  (round(prop_train*length(sequences), 0)+1) : length(sequences)\n",
        "\n",
        "\n",
        "    x_train <- pad_sequences(sequences[indices[train_indices]], maxlen = maxlen)\n",
        "    x_test <- pad_sequences(sequences[indices[test_indices]], maxlen = maxlen)\n",
        "\n",
        "    db_ys <- db_textos_splited[ , \"id\"] %>%\n",
        "      as.data.frame() %>%\n",
        "      set_names(\"id\") %>%\n",
        "      mutate(id = as.numeric(id)) %>%\n",
        "      left_join( {db_bfi %>% select(id, O_rec:N_vlti_rec)}, by = \"id\" )\n",
        "\n",
        "\n",
        "    y_train <- as.matrix(db_ys[indices[train_indices] , 2:6])\n",
        "    y_test <-  as.matrix(db_ys[indices[test_indices] , 2:6])\n",
        "\n",
        "    dim(x_train)\n",
        "    dim(x_test)\n",
        "\n",
        "    dim(y_train)\n",
        "    dim(y_test)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "he6lOChvBH93",
        "outputId": "1cea5079-ddd7-4a75-89f4-2759ea3796f9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>9460</li><li>5</li></ol>\n"
            ],
            "text/markdown": "1. 9460\n2. 5\n\n\n",
            "text/latex": "\\begin{enumerate*}\n\\item 9460\n\\item 5\n\\end{enumerate*}\n",
            "text/plain": [
              "[1] 9460    5"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  max_words = 66128\n",
        "    embedding_dim = 600\n",
        "\n",
        "# bidirectional lstm model\n",
        "\n",
        "   model <- keras_model_sequential() %>%\n",
        "     layer_embedding(\n",
        "         input_dim = max_words + 1,\n",
        "         output_dim = embedding_dim ,\n",
        "         weights = list(embedding_matrix),\n",
        "         input_length = maxlen,\n",
        "         trainable = FALSE ) %>%\n",
        "    bidirectional(layer_lstm(units = 64,recurrent_dropout = 0.5, dropout =.5)) %>%\n",
        "    layer_dense(units = 5,  kernel_regularizer = regularizer_l2(0.001))\n",
        "\n",
        "    summary(model)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzIhYyJ0BL4E",
        "outputId": "49a2711b-d4a4-4fca-b43e-eba9bf872aa2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "________________________________________________________________________________\n",
            " Layer (type)                  Output Shape               Param #    Trainable  \n",
            "================================================================================\n",
            " embedding_1 (Embedding)       (None, 250, 600)           39677400   N          \n",
            " bidirectional_1 (Bidirection  (None, 128)                340480     Y          \n",
            " al)                                                                            \n",
            " dense_1 (Dense)               (None, 5)                  645        Y          \n",
            "================================================================================\n",
            "Total params: 40018525 (152.66 MB)\n",
            "Trainable params: 341125 (1.30 MB)\n",
            "Non-trainable params: 39677400 (151.36 MB)\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# for continouse y-variable\n",
        "\n",
        "\n",
        "class_callback_loss_history <- new_callback_class(\n",
        "    \"loss_history\",\n",
        "    on_batch_end = function(batch, logs) {\n",
        "      print(paste0(\"Loss in batch \", batch, \": \", logs$get(\"loss\")))\n",
        "    }\n",
        ")\n",
        "\n",
        "    model %>% compile(\n",
        "        optimizer = \"rmsprop\",\n",
        "        loss = \"mse\",\n",
        "        metrics = c(\"mae\")\n",
        "    )\n",
        "\n",
        "\n",
        "    history <- model %>% fit(\n",
        "      x_train,\n",
        "      y_train,\n",
        "      epochs =9,\n",
        "      batch_size = 100,\n",
        "      validation_data = list(x_test, y_test),\n",
        "\n",
        "      callbacks = list(class_callback_loss_history())\n",
        "    )\n",
        "\n",
        "    plot(history)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YlJSd0kIBPyK",
        "outputId": "5de87988-2fd8-4265-ceb1-364355670a2b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ERROR",
          "evalue": "ignored",
          "traceback": [
            "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1084, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 544, in minimize\n        self.apply_gradients(grads_and_vars)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 1229, in apply_gradients\n        grads_and_vars = self.aggregate_gradients(grads_and_vars)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 1191, in aggregate_gradients\n        return optimizer_utils.all_reduce_sum_gradients(grads_and_vars)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/utils.py\", line 33, in all_reduce_sum_gradients\n        filtered_grads_and_vars = filter_empty_gradients(grads_and_vars)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/utils.py\", line 77, in filter_empty_gradients\n        raise ValueError(\n\n    ValueError: No gradients provided for any variable: (['bidirectional_1/forward_lstm_1/lstm_cell/kernel:0', 'bidirectional_1/forward_lstm_1/lstm_cell/recurrent_kernel:0', 'bidirectional_1/forward_lstm_1/lstm_cell/bias:0', 'bidirectional_1/backward_lstm_1/lstm_cell/kernel:0', 'bidirectional_1/backward_lstm_1/lstm_cell/recurrent_kernel:0', 'bidirectional_1/backward_lstm_1/lstm_cell/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'bidirectional_1/forward_lstm_1/lstm_cell/kernel:0' shape=(600, 256) dtype=float32>), (None, <tf.Variable 'bidirectional_1/forward_lstm_1/lstm_cell/recurrent_kernel:0' shape=(64, 256) dtype=float32>), (None, <tf.Variable 'bidirectional_1/forward_lstm_1/lstm_cell/bias:0' shape=(256,) dtype=float32>), (None, <tf.Variable 'bidirectional_1/backward_lstm_1/lstm_cell/kernel:0' shape=(600, 256) dtype=float32>), (None, <tf.Variable 'bidirectional_1/backward_lstm_1/lstm_cell/recurrent_kernel:0' shape=(64, 256) dtype=float32>), (None, <tf.Variable 'bidirectional_1/backward_lstm_1/lstm_cell/bias:0' shape=(256,) dtype=float32>), (None, <tf.Variable 'dense_1/kernel:0' shape=(128, 5) dtype=float32>), (None, <tf.Variable 'dense_1/bias:0' shape=(5,) dtype=float32>)).\nTraceback:\n",
            "1. model %>% fit(x_train, y_train, epochs = 9, batch_size = 100, \n .     validation_data = list(x_test, y_test), callbacks = list(class_callback_loss_history()))",
            "2. fit(., x_train, y_train, epochs = 9, batch_size = 100, validation_data = list(x_test, \n .     y_test), callbacks = list(class_callback_loss_history()))",
            "3. fit.keras.engine.training.Model(., x_train, y_train, epochs = 9, \n .     batch_size = 100, validation_data = list(x_test, y_test), \n .     callbacks = list(class_callback_loss_history()))",
            "4. do.call(object$fit, args)",
            "5. (structure(function (x = NULL, y = NULL, batch_size = NULL, epochs = 1L, \n .     verbose = \"auto\", callbacks = NULL, validation_split = 0, \n .     validation_data = NULL, shuffle = TRUE, class_weight = NULL, \n .     sample_weight = NULL, initial_epoch = 0L, steps_per_epoch = NULL, \n .     validation_steps = NULL, validation_batch_size = NULL, validation_freq = 1L, \n .     max_queue_size = 10L, workers = 1L, use_multiprocessing = FALSE) \n . {\n .     cl <- sys.call()\n .     cl[[1L]] <- list2\n .     call_args <- split_named_unnamed(eval(cl, parent.frame()))\n .     result <- py_call_impl(callable, call_args$unnamed, call_args$named)\n .     if (convert) \n .         result <- py_to_r(result)\n .     if (is.null(result)) \n .         invisible(result)\n .     else result\n . }, class = c(\"python.builtin.method\", \"python.builtin.object\"\n . ), py_object = <environment>))(batch_size = 100L, epochs = 9L, \n .     verbose = \"auto\", validation_split = 0, shuffle = TRUE, class_weight = NULL, \n .     sample_weight = NULL, initial_epoch = 0L, x = <environment>, \n .     y = <environment>, validation_data = <environment>, callbacks = list(\n .         <environment>, <environment>))",
            "6. py_call_impl(callable, call_args$unnamed, call_args$named)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    results <-  model %>% evaluate(x_test, y_test)\n",
        "    predictions <-  model %>% predict(x_test)\n",
        "    results\n",
        "\n",
        "    cbind(predictions, y_test) %>%  corr.test()\n",
        "\n",
        "    ggplot(\n",
        "        data = data.frame(\n",
        "            cbind(y_test, y_pred = predictions[ , 1])\n",
        "            ),\n",
        "        aes(x = y_test, y = y_pred) ) +\n",
        "        geom_point( alpha = 1/2) +\n",
        "        geom_smooth(method = \"lm\") +\n",
        "        geom_smooth(color = \"red\")\n",
        "\n",
        "   resp2[indices[test_indices], ] %>%\n",
        "       cbind(predictions) %>%\n",
        "       select(Código, Ma_measure, y_theta_z, predictions) %>%\n",
        "       group_by(Código) %>%\n",
        "       summarise_all(.funs = mean) %>%\n",
        "       select(-Código) %>%\n",
        "       ggplot(aes(x = Ma_measure, y = predictions) ) +\n",
        "        geom_point( alpha = 1/2) +\n",
        "        geom_smooth(method = \"lm\") +\n",
        "        geom_smooth(color = \"red\")\n",
        "\n",
        "\n",
        "    resp2[indices[test_indices], ] %>%\n",
        "       cbind(predictions) %>%\n",
        "       select(Código, Ma_measure, y_theta_z, predictions) %>%\n",
        "       group_by(Código) %>%\n",
        "       summarise_all(.funs = mean) %>%\n",
        "       select(-Código) %>%\n",
        "       corr.test()\n",
        "\n",
        "\n",
        "    results <- model %>% evaluate(x_train, y_train)\n",
        "    predictions <- model %>% predict(x_train)\n",
        "    cor(predictions, y_train)\n",
        "\n",
        "    ggplot(\n",
        "        data = data.frame(\n",
        "            cbind(y_train, y_pred = predictions[ , 1])\n",
        "            ),\n",
        "        aes(x = y_train, y = y_pred) ) +\n",
        "        geom_point( alpha = 1/2) +\n",
        "        geom_smooth(color = \"orange\") +\n",
        "        geom_smooth(method = \"lm\")\n",
        "\n"
      ],
      "metadata": {
        "id": "fzkQZjTCBU2x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}